#!/usr/bin/env python3
"""
å…±é€šã®LLM APIå‘¼ã³å‡ºã—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã€LLMã®å¿œç­”ã‚’å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚

ä½¿ç”¨ä¾‹:
    cp .env.example .env && vi .env  # APIã‚­ãƒ¼ã‚’è¨­å®š
    uv run python call-llm.py 2-1-2
    uv run python call-llm.py 2-1-2 --temperature 1.5
    uv run python call-llm.py 2-1-2 --system "ã‚ãªãŸã¯å°‚é–€å®¶ã§ã™"
"""

import openai
import os
import sys
import json
import argparse
from pathlib import Path
from typing import Optional, Dict, Any

from dotenv import load_dotenv

load_dotenv()

PROMPTS_DIR = Path("prompts")
OUTPUTS_DIR = Path("outputs")

PROMPTS_DIR.mkdir(exist_ok=True)
OUTPUTS_DIR.mkdir(exist_ok=True)


def read_prompt_file(file_id: str) -> Dict[str, Any]:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        file_id: ãƒ•ã‚¡ã‚¤ãƒ«ID (ä¾‹: "2-1-2")
    
    Returns:
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã®è¾æ›¸
    """
    prompt_file = PROMPTS_DIR / f"{file_id}-prompt.txt"
    
    if not prompt_file.exists():
        raise FileNotFoundError(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {prompt_file}")
    
    content = prompt_file.read_text(encoding="utf-8")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†é›¢
    lines = content.split("\n")
    metadata = {}
    prompt_start = 0
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è§£æï¼ˆ---ã§å›²ã¾ã‚ŒãŸéƒ¨åˆ†ï¼‰
    if lines[0] == "---":
        for i, line in enumerate(lines[1:], 1):
            if line == "---":
                prompt_start = i + 1
                break
            if ":" in line:
                key, value = line.split(":", 1)
                metadata[key.strip()] = value.strip()
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬æ–‡ã‚’å–å¾—
    prompt = "\n".join(lines[prompt_start:]).strip()
    
    return {
        "prompt": prompt,
        "metadata": metadata
    }

def call_llm(
    prompt: str,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 500,
    model: str = "gpt-5-nano"
) -> str:
    """
    LLM APIã‚’å‘¼ã³å‡ºã™
    
    Args:
        prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        temperature: ç”Ÿæˆã®å¤šæ§˜æ€§
        max_tokens: æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
    
    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    api_key = os.getenv("OPENAI_API_KEY")
    
    if not api_key or api_key == "your-api-key-here":
        print("âš ï¸ OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("  1. .env.example ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ .env ã‚’ä½œæˆ")
        print("  2. .env ã« OPENAI_API_KEY ã‚’è¨­å®š")
        return "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‡ºåŠ›ï¼šAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€å®Ÿéš›ã®LLMå¿œç­”ã¯å–å¾—ã§ãã¾ã›ã‚“ã€‚"
    
    client = openai.OpenAI(api_key=api_key)
    
    messages = []
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    
    try:
        # ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š
        completion_params = {
            "model": model,
            "messages": messages
        }

        # gpt-5-nanoç³»ã¯ temperatureã¨max_tokensã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å—ã‘ä»˜ã‘ãªã„
        if "gpt-5" in model:
            # gpt-5-nanoã¯è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—
            pass
        else:
            # ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            completion_params["temperature"] = temperature
            completion_params["max_tokens"] = max_tokens

        response = client.chat.completions.create(**completion_params)
        return response.choices[0].message.content
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

def save_output(file_id: str, output: str, metadata: Dict[str, Any]):
    """
    å‡ºåŠ›ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        file_id: ãƒ•ã‚¡ã‚¤ãƒ«ID
        output: LLMã®å‡ºåŠ›
        metadata: å®Ÿè¡Œæ™‚ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    """
    OUTPUTS_DIR.mkdir(exist_ok=True)
    output_file = OUTPUTS_DIR / f"{file_id}-out.txt"
    
    content = []
    content.append("---")
    for key, value in metadata.items():
        content.append(f"{key}: {value}")
    content.append("---")
    content.append("")
    content.append(output)
    
    output_file.write_text("\n".join(content), encoding="utf-8")
    print(f"âœ… å‡ºåŠ›ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

def main():
    parser = argparse.ArgumentParser(
        description="LLM APIã‚’å‘¼ã³å‡ºã—ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å®Ÿè¡Œ"
    )
    parser.add_argument(
        "file_id",
        help="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ID (ä¾‹: 2-1-2)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=None,
        help="Temperature ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (0.0-2.0)"
    )
    parser.add_argument(
        "--system",
        type=str,
        default=None,
        help="ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=None,
        help="æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=None,
        help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«"
    )
    parser.add_argument(
        "--repeat",
        type=int,
        default=1,
        help="ç¹°ã‚Šè¿”ã—å®Ÿè¡Œå›æ•°"
    )
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    # args.modelã¯å¾Œã§file_metadataã¨çµ„ã¿åˆã‚ã›ã¦å‡¦ç†ã™ã‚‹ã®ã§ã“ã“ã§ã¯ã‚»ãƒƒãƒˆã—ãªã„
    if args.max_tokens is None:
        args.max_tokens = int(os.getenv("DEFAULT_MAX_TOKENS", "500"))
    
    print(f"{'='*60}")
    print(f"LLM API å‘¼ã³å‡ºã—: {args.file_id}")
    print(f"{'='*60}")
    
    try:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        prompt_data = read_prompt_file(args.file_id)
        prompt = prompt_data["prompt"]
        file_metadata = prompt_data["metadata"]
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’å–å¾—
        temperature = args.temperature or float(file_metadata.get("temperature", 0.7))
        system_prompt = args.system or file_metadata.get("system", None)
        model = args.model or file_metadata.get("model", os.getenv("OPENAI_MODEL", "gpt-5-nano"))

        # å®Ÿè¡Œå›æ•°: ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒ1ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã®å ´åˆã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—
        if args.repeat == 1 and "executions" in file_metadata:
            args.repeat = int(file_metadata.get("executions", 1))
        
        print(f"\nğŸ“„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
        print("-" * 40)
        print(prompt[:200] + "..." if len(prompt) > 200 else prompt)
        print("-" * 40)
        
        if system_prompt:
            print(f"\nğŸ”§ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:")
            print("-" * 40)
            print(system_prompt[:200] + "..." if len(system_prompt) > 200 else system_prompt)
            print("-" * 40)
        
        print(f"\nâš™ï¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  - Temperature: {temperature}")
        print(f"  - Max Tokens: {args.max_tokens}")
        print(f"  - Model: {model}")
        print(f"  - å®Ÿè¡Œå›æ•°: {args.repeat}")
        
        # LLMã‚’å‘¼ã³å‡ºã™
        outputs = []
        for i in range(args.repeat):
            if args.repeat > 1:
                print(f"\nğŸ”„ å®Ÿè¡Œ {i+1}/{args.repeat}")
            
            output = call_llm(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=temperature,
                max_tokens=args.max_tokens,
                model=model
            )
            outputs.append(output)
            
            print(f"\nğŸ’¬ å‡ºåŠ›{i+1 if args.repeat > 1 else ''}:")
            print("-" * 40)
            print(output)
            print("-" * 40)
        
        # å‡ºåŠ›ã‚’ä¿å­˜
        execution_metadata = {
            "model": model,
            "executions": args.repeat,
            "has_system_prompt": "yes" if system_prompt else "no"
        }

        # gpt-5-nanoä»¥å¤–ã¯temperatureã¨max_tokensã‚‚è¨˜éŒ²
        if "gpt-5" not in model:
            execution_metadata["temperature"] = temperature
            execution_metadata["max_tokens"] = args.max_tokens
        
        if args.repeat == 1:
            save_output(args.file_id, outputs[0], execution_metadata)
        else:
            # è¤‡æ•°å®Ÿè¡Œã®å ´åˆã¯ã™ã¹ã¦ã®å‡ºåŠ›ã‚’ä¿å­˜
            combined_output = "\n\n=== å®Ÿè¡Œã”ã¨ã®å‡ºåŠ› ===\n\n".join(
                [f"[å®Ÿè¡Œ {i+1}]\n{out}" for i, out in enumerate(outputs)]
            )
            save_output(args.file_id, combined_output, execution_metadata)
        
    except FileNotFoundError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ« '{PROMPTS_DIR / (args.file_id + '-prompt.txt')}' ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
        print("\nä¾‹:")
        print("---")
        print("temperature: 0.7")
        print("system: ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™")
        print("---")
        print("ã“ã“ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨˜è¼‰")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
